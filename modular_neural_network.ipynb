{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff5474b",
   "metadata": {},
   "source": [
    "# Modular Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8b9aa",
   "metadata": {},
   "source": [
    "## The Layer Class\n",
    "\n",
    "The `Layer` class is the fundamental parent class used in the modular implementation of a neural network using stochastic gradient descent and backpropogation.  \n",
    "\n",
    "Each layer acts as a function, with given input/output sizes when required. Each layer must include the following methods:  \n",
    "* `forward`: calculates the output of the layer given input, $ x $.\n",
    "* `backward`: given the derivative of the error with respect to the layer's output, $ \\frac{\\partial E}{\\partial y} $, the backward function must calculate the derivative of the error with respect to the layer's parameters, $ \\frac{\\partial E}{\\partial w} $, and return the derivative of the error with respect to the layer's input, $ \\frac{\\partial E}{\\partial x} $.\n",
    "* `update_params`: updates the layer's parameters based on the the derivatives calculated in the backward method, according to the equation $ w' = w - \\frac{\\eta}{n}\\frac{\\partial E}{\\partial w} $. Note that for layers which have no tunable parameters, the update params method is left blank.\n",
    "\n",
    "Example layers include the Linear and Sigmoid layers, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0312f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, nabla_out):\n",
    "        pass\n",
    "    \n",
    "    def update_params(self, eta, n):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size) / np.sqrt(input_size)\n",
    "        self.biases = np.random.randn(output_size, 1)\n",
    "        self.nabla_w = 0\n",
    "        self.nabla_b = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.dot(self.weights, self.input) + self.biases\n",
    "    \n",
    "    def backward(self, nabla_out):\n",
    "        self.nabla_w += np.dot(nabla_out, self.input.T)\n",
    "        self.nabla_b += nabla_out\n",
    "        return np.dot(self.weights.T, nabla_out)\n",
    "    \n",
    "    def update_params(self, eta, n):\n",
    "        self.weights -= eta * self.nabla_w / n\n",
    "        self.biases -= eta * self.nabla_b / n\n",
    "        self.nabla_w = 0\n",
    "        self.nabla_b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a676c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        self.func = lambda x: 1 / (1 + np.exp(-x))\n",
    "        self.func_prime = lambda x: self.func(x) * (1 - self.func(x))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return self.func(self.input)\n",
    "    \n",
    "    def backward(self, nabla_out):\n",
    "        return np.multiply(nabla_out, self.func_prime(self.input))\n",
    "    \n",
    "    def update_params(self, eta, n):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c33e9",
   "metadata": {},
   "source": [
    "## The Network Class\n",
    "\n",
    "The `Network` class is inialized with a list of layer modules which define the architecture of the network. Analogously to the `forward`, `backward`, and `update_params` methods, the `Network` class contains the `forward_prop`, `backward_prop`, and `update_params` methods, which sequentially call the respective method in each of the layers of the network.  \n",
    "\n",
    "The `Network` class includes several accuracy evaluation methods such as `evaluate_percentage_onehot` and `evaluate_error`. Each of these methods evaluates the performance of the network on a given set of test data and labels by calculating the percentage of properly classified test points or directly calculating the error as defined by the network's error function.  \n",
    "\n",
    "The `mse` method implements the mean squared error function, defined as $ MSE = \\frac{1}{N} \\sum_{i=1}^{N}(y_i - \\hat{y_i})^2 $ and was chosen as the network's error function. The `mse_prime` method implements the derivative of the mean squared error function with respect to the actual output, $ y $. Note that this error function could be replaced with another error function of choice.  \n",
    "\n",
    "The `train` function tunes the network parameters according to the stochastic gradient descent algorithm. For each epoch, the input data and labels are split into mini batches. For every data point in the mini batch, the output and error are calculated, then the gradients are calculated using the backpropogation algorithm and the parameters are updated accordingly. The `train` method takes in the following arguments:  \n",
    "* `input_data`: the input training data used to train the model\n",
    "* `labels`: the input training labels used to train the model\n",
    "* `mini_batch_size`: the size of each mini batch used during stochastic gradient descent\n",
    "* `eta`: the learning rate for the stochastic gradient descent algorithm\n",
    "* `epochs`: the number of training epochs to run\n",
    "* `epoch_disp`: the number of epochs to display between each network evalution\n",
    "* `evaluation`: set the network accuracy evaluation function\n",
    "* `test_data`: the test data to use for network accuracy evaluations\n",
    "* `test_labels`: the test labels to use for network accuracy evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward_prop(self, x):\n",
    "        y = x\n",
    "        for layer in self.layers:\n",
    "            y = layer.forward(y)\n",
    "        return y\n",
    "    \n",
    "    def backward_prop(self, nabla):\n",
    "        for layer in reversed(self.layers):\n",
    "            nabla = layer.backward(nabla)\n",
    "        return nabla\n",
    "            \n",
    "    def update_params(self, eta, n):\n",
    "        for layer in self.layers:\n",
    "            layer.update_params(eta, n)\n",
    "    \n",
    "    def train(self, input_data, labels, mini_batch_size, eta, epochs, epoch_disp, evaluation='error',\n",
    "        test_data=None, test_labels=None):\n",
    "        n = len(input_data)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "\n",
    "            p = np.random.permutation(n)\n",
    "            input_data = input_data[p]\n",
    "            labels = labels[p]\n",
    "            \n",
    "            mini_batch_inputs = [input_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            mini_batch_labels = [labels[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            for i in range(len(mini_batch_inputs)):\n",
    "                \n",
    "                mini_batch_input = mini_batch_inputs[i]\n",
    "                mini_batch_label = mini_batch_labels[i]\n",
    "                \n",
    "                for x, y in zip(mini_batch_input, mini_batch_label):\n",
    "                    output = self.forward_prop(x)\n",
    "                    nabla = self.mse_prime(output, y)\n",
    "                    self.backward_prop(nabla)\n",
    "            \n",
    "                self.update_params(eta, mini_batch_size)\n",
    "            \n",
    "            if (test_data is not None) & (epoch_disp != 0):\n",
    "                \n",
    "                if (e % epoch_disp == 0) | (e == epochs - 1):\n",
    "\n",
    "                    if evaluation == 'error':\n",
    "                        err = self.evaluate_error(test_data, test_labels)\n",
    "                        print(f'Error in epoch {e+1} / {epochs}: {round(err, 5)}')\n",
    "                    elif evaluation == 'percentage':\n",
    "                        acc = self.evaluate_percentage(test_data, test_labels)\n",
    "                        print(f'Accuracy in epoch {e+1} / {epochs}: {round(acc, 2)}%')\n",
    "                    elif evaluation == 'percentage_onehot':\n",
    "                        acc = self.evaluate_percentage_onehot(test_data, test_labels)\n",
    "                        print(f'Accuracy in epoch {e+1} / {epochs}: {round(acc, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637ec5b",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "The above code along with several other implementations of the `Layer` class is included in the `network.py` module. An additional module, `network_utils.py` is also included to facilitate experimentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
